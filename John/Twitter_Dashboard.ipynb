{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import os\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "import tweepy                                                                       # pip install tweepy\r\n",
    "from tweepy import Stream\r\n",
    "from tweepy import OAuthHandler\r\n",
    "from tweepy.streaming import StreamListener\r\n",
    "import gspread\r\n",
    "#from oauth2client.service_account import ServiceAccountCredentials\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "import time\r\n",
    "import re\r\n",
    "import preprocessor as p                                                             # pip install tweet-preprocessor                                                    \r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from textblob import TextBlob                                                        # pip install textblob \r\n",
    "from nltk.corpus import stopwords\r\n",
    "import pygsheets\r\n",
    "from tkinter import *\r\n",
    "import pycountry\r\n",
    "from tkinter import messagebox\r\n",
    "from tkinter import ttk\r\n",
    "import webbrowser \r\n",
    "from dotenv import load_dotenv\r\n",
    "load_dotenv('john.env')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "consumer_key = os.getenv('TWITTER_API')\r\n",
    "consumer_secret = os.getenv('TWITTER_SECRET_KEY')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "type(consumer_key)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "callback_uri = 'oob'                                                      # declare a uniform resource indicator, and set to 'out-of-band'\r\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret, callback_uri)   # creates an authentication object. stores our consumer and secret key. \r\n",
    "redirect_url = auth.get_authorization_url()                               # stores the API authorization url used to get a PIN number                             \r\n",
    "webbrowser.open(redirect_url)                                             # Opens web and redirects you to Twitter app authorization website. Click Authorize and it provides a PIN number.\r\n",
    "prompt = '>'                                                              # Enter the PIN number here. \r\n",
    "print('enter PIN number at the prompt. Hit enter.')\r\n",
    "user_PIN = input(prompt)                                                  # enter the PIN number in prompt and now you'll have an access token. \r\n",
    "auth.get_access_token(user_PIN)                                           # Uses the PIN to generate an access token.\r\n",
    "print('Your access key ', auth.access_token)                              # This key does not change regardless how many different PIN numbers you get.\r\n",
    "print('The secret key ', auth.access_token_secret)\r\n",
    "api = tweepy.API(auth)                                                    # Create a tweepy API object with the access token. Set wait_on_rate_limit to reset after 15 minutes."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "enter PIN number at the prompt. Hit enter.\n",
      "Your access key  18325825-cHvbx5Kpg6HFjIrFYo8wfr8jheadKCXjUNQG57BvU\n",
      "The secret key  Bk14J5TH5g4YmZvRmra7u23uwrkfwgWrNHHs0v4G0Ttby\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "api.update_status(status='Test')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Status(_api=<tweepy.api.API object at 0x000002189A404BC8>, _json={'created_at': 'Sun Aug 01 23:31:06 +0000 2021', 'id': 1421976817099952129, 'id_str': '1421976817099952129', 'text': 'Test', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}, 'source': '<a href=\"https://help.twitter.com/en/using-twitter/how-to-tweet#source-labels\" rel=\"nofollow\">FINTECH_CLASS_UW</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 18325825, 'id_str': '18325825', 'name': 'jb_bees', 'screen_name': 'JB_Bees', 'location': 'PA', 'description': 'none of it has been my fault so far.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 17, 'friends_count': 25, 'listed_count': 1, 'created_at': 'Tue Dec 23 03:57:12 +0000 2008', 'favourites_count': 229, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 1760, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '9AE4E8', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/18325825/1622083053', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'BDDCAD', 'profile_sidebar_fill_color': 'DDFFCC', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'lang': 'en'}, created_at=datetime.datetime(2021, 8, 1, 23, 31, 6), id=1421976817099952129, id_str='1421976817099952129', text='Test', truncated=False, entities={'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': []}, source='FINTECH_CLASS_UW', source_url='https://help.twitter.com/en/using-twitter/how-to-tweet#source-labels', in_reply_to_status_id=None, in_reply_to_status_id_str=None, in_reply_to_user_id=None, in_reply_to_user_id_str=None, in_reply_to_screen_name=None, author=User(_api=<tweepy.api.API object at 0x000002189A404BC8>, _json={'id': 18325825, 'id_str': '18325825', 'name': 'jb_bees', 'screen_name': 'JB_Bees', 'location': 'PA', 'description': 'none of it has been my fault so far.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 17, 'friends_count': 25, 'listed_count': 1, 'created_at': 'Tue Dec 23 03:57:12 +0000 2008', 'favourites_count': 229, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 1760, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '9AE4E8', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/18325825/1622083053', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'BDDCAD', 'profile_sidebar_fill_color': 'DDFFCC', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, id=18325825, id_str='18325825', name='jb_bees', screen_name='JB_Bees', location='PA', description='none of it has been my fault so far.', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=17, friends_count=25, listed_count=1, created_at=datetime.datetime(2008, 12, 23, 3, 57, 12), favourites_count=229, utc_offset=None, time_zone=None, geo_enabled=True, verified=False, statuses_count=1760, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='9AE4E8', profile_background_image_url='http://abs.twimg.com/images/themes/theme16/bg.gif', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme16/bg.gif', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/18325825/1622083053', profile_link_color='0084B4', profile_sidebar_border_color='BDDCAD', profile_sidebar_fill_color='DDFFCC', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', withheld_in_countries=[]), user=User(_api=<tweepy.api.API object at 0x000002189A404BC8>, _json={'id': 18325825, 'id_str': '18325825', 'name': 'jb_bees', 'screen_name': 'JB_Bees', 'location': 'PA', 'description': 'none of it has been my fault so far.', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 17, 'friends_count': 25, 'listed_count': 1, 'created_at': 'Tue Dec 23 03:57:12 +0000 2008', 'favourites_count': 229, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 1760, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '9AE4E8', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme16/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/18325825/1622083053', 'profile_link_color': '0084B4', 'profile_sidebar_border_color': 'BDDCAD', 'profile_sidebar_fill_color': 'DDFFCC', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'withheld_in_countries': []}, id=18325825, id_str='18325825', name='jb_bees', screen_name='JB_Bees', location='PA', description='none of it has been my fault so far.', url=None, entities={'description': {'urls': []}}, protected=False, followers_count=17, friends_count=25, listed_count=1, created_at=datetime.datetime(2008, 12, 23, 3, 57, 12), favourites_count=229, utc_offset=None, time_zone=None, geo_enabled=True, verified=False, statuses_count=1760, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='9AE4E8', profile_background_image_url='http://abs.twimg.com/images/themes/theme16/bg.gif', profile_background_image_url_https='https://abs.twimg.com/images/themes/theme16/bg.gif', profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1369806890503270401/Ng0ZV3q5_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/18325825/1622083053', profile_link_color='0084B4', profile_sidebar_border_color='BDDCAD', profile_sidebar_fill_color='DDFFCC', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=False, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', withheld_in_countries=[]), geo=None, coordinates=None, place=None, contributors=None, is_quote_status=False, retweet_count=0, favorite_count=0, favorited=False, retweeted=False, lang='en')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#HappyEmoticons\r\n",
    "emoticons_happy = set([\r\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\r\n",
    "   ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\r\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\r\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\r\n",
    "    '<3'\r\n",
    "    ])\r\n",
    "\r\n",
    "# Sad Emoticons\r\n",
    "emoticons_sad = set([\r\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\r\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\r\n",
    "    ':c', ':{', '>:\\\\', ';('\r\n",
    "    ])\r\n",
    "\r\n",
    "#Emoji patterns\r\n",
    "emoji_pattern = re.compile(\"[\"\r\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "         u\"\\U00002702-\\U000027B0\"\r\n",
    "         u\"\\U000024C2-\\U0001F251\"\r\n",
    "         \"]+\", flags=re.UNICODE)\r\n",
    "\r\n",
    "#combine sad and happy emoticons\r\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def clean_tweets(tweet):\r\n",
    "\r\n",
    "    stop_words = set(stopwords.words('english'))\r\n",
    "    word_tokens = word_tokenize(tweet)\r\n",
    "\r\n",
    "    tweet = re.sub(r':', '', tweet)\r\n",
    "    tweet = re.sub(r'[~\\x00-\\x7F]+', '', tweet)\r\n",
    "\r\n",
    "\r\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\r\n",
    "\r\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\r\n",
    "    filtered_tweet = []\r\n",
    "\r\n",
    "    for w in word_tokens:\r\n",
    "\r\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\r\n",
    "            filtered_tweet.append(w)\r\n",
    "\r\n",
    "    return ' '.join(filtered_tweet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Start of GUI\r\n",
    "window = Tk()\r\n",
    "#Title of Gui\r\n",
    "window.title(\"Sentiment Analysis\")\r\n",
    "#multiple tabs\r\n",
    "tab_control = ttk.Notebook(window)\r\n",
    "\r\n",
    "tab1 = ttk.Frame(tab_control)\r\n",
    "\r\n",
    "tab2 = ttk.Frame(tab_control)\r\n",
    "\r\n",
    "tab_control.add(tab1, text='System')\r\n",
    "\r\n",
    "tab_control.add(tab2, text='About')\r\n",
    "#GUI window size\r\n",
    "window.geometry('1000x700')\r\n",
    "heading = Label(tab1, text=\"Welcome To Sentiment Analysis of tweets\", font=(\"Arial Bold\", 25))\r\n",
    "#GUI fields position\r\n",
    "heading.grid(column=1, row=0)\r\n",
    "lbl = Label(tab1, text=\"Please enter your keyword\", font=(\"Times New Roman\", 20))\r\n",
    "lbl.grid(column=1, row=10)\r\n",
    "txt = Entry(tab1,width=10)\r\n",
    "\r\n",
    "txt.grid(column=2, row=10)\r\n",
    "prgbar = Label(tab1, text=\"Current status: Awaiting input from user\", font=(\"Times New Roman\", 10))                        # progress bar visual\r\n",
    "prgbar.grid(column=3,row =12)\r\n",
    "\r\n",
    "#tab2\r\n",
    "abt = Label(tab2,text=\"Welcome To Sentiment Analysis of tweets\", font=(\"Arial Bold\", 25))\r\n",
    "abt.grid(column=1, row=0)\r\n",
    "abt2 = Label(tab2,text=\"Good Day\", font=(\"Times New Roman\", 15))\r\n",
    "abt2.grid(column=1, row=1)\r\n",
    "abt3 = Label(tab2,text=\"This program takes an input of a keyword from user and extracts latest 200-250 tweets relating to the keyword.\", font=(\"Times New Roman\", 15))\r\n",
    "abt3.grid(column=1, row=2)\r\n",
    "abt4 = Label(tab2,text=\"After extraction these tweets are analyzed exploratory and sentimentally before being presented visually\", font=(\"Times New Roman\", 15))\r\n",
    "abt4.grid(column=1, row=3)\r\n",
    "abt5 = Label(tab2,text=\"For more information please contact: Name\", font=(\"Times New Roman\", 15))\r\n",
    "abt5.grid(column=1, row=4)\r\n",
    "abt6 = Label(tab2,text=\"Email: \", font=(\"Times New Roman\", 10))\r\n",
    "abt6.grid(column=1, row=5)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#process once the button is clicked\r\n",
    "def clicked():\r\n",
    "    messagebox.showinfo('Message', 'The analysis has begun')\r\n",
    "\r\n",
    "    prgbar.configure(text=\"Current status: extracting tweets\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#extract keyword from user\r\n",
    "search_words = 'dogecoin'\r\n",
    "\r\n",
    "# Collect tweets\r\n",
    "tweets = tweepy.Cursor(\r\n",
    "    api.search,\r\n",
    "    q=search_words,\r\n",
    "    lang=\"en\"\r\n",
    "    ).items(500)\r\n",
    "\r\n",
    "#print extraction of tweets\r\n",
    "tweets_copy = []\r\n",
    "for tweet in tqdm(tweets):\r\n",
    "    tweets_copy.append(tweet)\r\n",
    "    print(f\"new tweets retrieved: {len(tweets_copy)}\")\r\n",
    "    prgbar.configure(text=f\"Current status: Tweets extracted, {len(tweets_copy)}\" )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1it [00:00,  3.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 1\n",
      "new tweets retrieved: 2\n",
      "new tweets retrieved: 3\n",
      "new tweets retrieved: 4\n",
      "new tweets retrieved: 5\n",
      "new tweets retrieved: 6\n",
      "new tweets retrieved: 7\n",
      "new tweets retrieved: 8\n",
      "new tweets retrieved: 9\n",
      "new tweets retrieved: 10\n",
      "new tweets retrieved: 11\n",
      "new tweets retrieved: 12\n",
      "new tweets retrieved: 13\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "14it [00:00, 21.08it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 14\n",
      "new tweets retrieved: 15\n",
      "new tweets retrieved: 16\n",
      "new tweets retrieved: 17\n",
      "new tweets retrieved: 18\n",
      "new tweets retrieved: 19\n",
      "new tweets retrieved: 20\n",
      "new tweets retrieved: 21\n",
      "new tweets retrieved: 22\n",
      "new tweets retrieved: 23\n",
      "new tweets retrieved: 24\n",
      "new tweets retrieved: 25\n",
      "new tweets retrieved: 26\n",
      "new tweets retrieved: 27\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "28it [00:01, 26.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 28\n",
      "new tweets retrieved: 29\n",
      "new tweets retrieved: 30\n",
      "new tweets retrieved: 31\n",
      "new tweets retrieved: 32\n",
      "new tweets retrieved: 33\n",
      "new tweets retrieved: 34\n",
      "new tweets retrieved: 35\n",
      "new tweets retrieved: 36\n",
      "new tweets retrieved: 37\n",
      "new tweets retrieved: 38\n",
      "new tweets retrieved: 39\n",
      "new tweets retrieved: 40\n",
      "new tweets retrieved: 41\n",
      "new tweets retrieved: 42\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "43it [00:01, 32.94it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 43\n",
      "new tweets retrieved: 44\n",
      "new tweets retrieved: 45\n",
      "new tweets retrieved: 46\n",
      "new tweets retrieved: 47\n",
      "new tweets retrieved: 48\n",
      "new tweets retrieved: 49\n",
      "new tweets retrieved: 50\n",
      "new tweets retrieved: 51\n",
      "new tweets retrieved: 52\n",
      "new tweets retrieved: 53\n",
      "new tweets retrieved: 54\n",
      "new tweets retrieved: 55\n",
      "new tweets retrieved: 56\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "57it [00:01, 34.81it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 57\n",
      "new tweets retrieved: 58\n",
      "new tweets retrieved: 59\n",
      "new tweets retrieved: 60\n",
      "new tweets retrieved: 61\n",
      "new tweets retrieved: 62\n",
      "new tweets retrieved: 63\n",
      "new tweets retrieved: 64\n",
      "new tweets retrieved: 65\n",
      "new tweets retrieved: 66\n",
      "new tweets retrieved: 67\n",
      "new tweets retrieved: 68\n",
      "new tweets retrieved: 69\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "70it [00:02, 35.24it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 70\n",
      "new tweets retrieved: 71\n",
      "new tweets retrieved: 72\n",
      "new tweets retrieved: 73\n",
      "new tweets retrieved: 74\n",
      "new tweets retrieved: 75\n",
      "new tweets retrieved: 76\n",
      "new tweets retrieved: 77\n",
      "new tweets retrieved: 78\n",
      "new tweets retrieved: 79\n",
      "new tweets retrieved: 80\n",
      "new tweets retrieved: 81\n",
      "new tweets retrieved: 82\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "83it [00:02, 35.46it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 83\n",
      "new tweets retrieved: 84\n",
      "new tweets retrieved: 85\n",
      "new tweets retrieved: 86\n",
      "new tweets retrieved: 87\n",
      "new tweets retrieved: 88\n",
      "new tweets retrieved: 89\n",
      "new tweets retrieved: 90\n",
      "new tweets retrieved: 91\n",
      "new tweets retrieved: 92\n",
      "new tweets retrieved: 93\n",
      "new tweets retrieved: 94\n",
      "new tweets retrieved: 95\n",
      "new tweets retrieved: 96\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "97it [00:02, 36.22it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 97\n",
      "new tweets retrieved: 98\n",
      "new tweets retrieved: 99\n",
      "new tweets retrieved: 100\n",
      "new tweets retrieved: 101\n",
      "new tweets retrieved: 102\n",
      "new tweets retrieved: 103\n",
      "new tweets retrieved: 104\n",
      "new tweets retrieved: 105\n",
      "new tweets retrieved: 106\n",
      "new tweets retrieved: 107\n",
      "new tweets retrieved: 108\n",
      "new tweets retrieved: 109\n",
      "new tweets retrieved: 110\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "111it [00:03, 34.82it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 111\n",
      "new tweets retrieved: 112\n",
      "new tweets retrieved: 113\n",
      "new tweets retrieved: 114\n",
      "new tweets retrieved: 115\n",
      "new tweets retrieved: 116\n",
      "new tweets retrieved: 117\n",
      "new tweets retrieved: 118\n",
      "new tweets retrieved: 119\n",
      "new tweets retrieved: 120\n",
      "new tweets retrieved: 121\n",
      "new tweets retrieved: 122\n",
      "new tweets retrieved: 123\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "124it [00:03, 35.95it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 124\n",
      "new tweets retrieved: 125\n",
      "new tweets retrieved: 126\n",
      "new tweets retrieved: 127\n",
      "new tweets retrieved: 128\n",
      "new tweets retrieved: 129\n",
      "new tweets retrieved: 130\n",
      "new tweets retrieved: 131\n",
      "new tweets retrieved: 132\n",
      "new tweets retrieved: 133\n",
      "new tweets retrieved: 134\n",
      "new tweets retrieved: 135\n",
      "new tweets retrieved: 136\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "137it [00:04, 36.12it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 137\n",
      "new tweets retrieved: 138\n",
      "new tweets retrieved: 139\n",
      "new tweets retrieved: 140\n",
      "new tweets retrieved: 141\n",
      "new tweets retrieved: 142\n",
      "new tweets retrieved: 143\n",
      "new tweets retrieved: 144\n",
      "new tweets retrieved: 145\n",
      "new tweets retrieved: 146\n",
      "new tweets retrieved: 147\n",
      "new tweets retrieved: 148\n",
      "new tweets retrieved: 149\n",
      "new tweets retrieved: 150\n",
      "new tweets retrieved: 151\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "152it [00:04, 36.78it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 152\n",
      "new tweets retrieved: 153\n",
      "new tweets retrieved: 154\n",
      "new tweets retrieved: 155\n",
      "new tweets retrieved: 156\n",
      "new tweets retrieved: 157\n",
      "new tweets retrieved: 158\n",
      "new tweets retrieved: 159\n",
      "new tweets retrieved: 160\n",
      "new tweets retrieved: 161\n",
      "new tweets retrieved: 162\n",
      "new tweets retrieved: 163\n",
      "new tweets retrieved: 164\n",
      "new tweets retrieved: 165\n",
      "new tweets retrieved: 166\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "167it [00:04, 39.40it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 167\n",
      "new tweets retrieved: 168\n",
      "new tweets retrieved: 169\n",
      "new tweets retrieved: 170\n",
      "new tweets retrieved: 171\n",
      "new tweets retrieved: 172\n",
      "new tweets retrieved: 173\n",
      "new tweets retrieved: 174\n",
      "new tweets retrieved: 175\n",
      "new tweets retrieved: 176\n",
      "new tweets retrieved: 177\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "178it [00:05, 38.15it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 178\n",
      "new tweets retrieved: 179\n",
      "new tweets retrieved: 180\n",
      "new tweets retrieved: 181\n",
      "new tweets retrieved: 182\n",
      "new tweets retrieved: 183\n",
      "new tweets retrieved: 184\n",
      "new tweets retrieved: 185\n",
      "new tweets retrieved: 186\n",
      "new tweets retrieved: 187\n",
      "new tweets retrieved: 188\n",
      "new tweets retrieved: 189\n",
      "new tweets retrieved: 190\n",
      "new tweets retrieved: 191\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "192it [00:05, 40.31it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 192\n",
      "new tweets retrieved: 193\n",
      "new tweets retrieved: 194\n",
      "new tweets retrieved: 195\n",
      "new tweets retrieved: 196\n",
      "new tweets retrieved: 197\n",
      "new tweets retrieved: 198\n",
      "new tweets retrieved: 199\n",
      "new tweets retrieved: 200\n",
      "new tweets retrieved: 201\n",
      "new tweets retrieved: 202\n",
      "new tweets retrieved: 203\n",
      "new tweets retrieved: 204\n",
      "new tweets retrieved: 205\n",
      "new tweets retrieved: 206\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "207it [00:05, 41.36it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 207\n",
      "new tweets retrieved: 208\n",
      "new tweets retrieved: 209\n",
      "new tweets retrieved: 210\n",
      "new tweets retrieved: 211\n",
      "new tweets retrieved: 212\n",
      "new tweets retrieved: 213\n",
      "new tweets retrieved: 214\n",
      "new tweets retrieved: 215\n",
      "new tweets retrieved: 216\n",
      "new tweets retrieved: 217\n",
      "new tweets retrieved: 218\n",
      "new tweets retrieved: 219\n",
      "new tweets retrieved: 220\n",
      "new tweets retrieved: 221\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "222it [00:06, 43.03it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 222\n",
      "new tweets retrieved: 223\n",
      "new tweets retrieved: 224\n",
      "new tweets retrieved: 225\n",
      "new tweets retrieved: 226\n",
      "new tweets retrieved: 227\n",
      "new tweets retrieved: 228\n",
      "new tweets retrieved: 229\n",
      "new tweets retrieved: 230\n",
      "new tweets retrieved: 231\n",
      "new tweets retrieved: 232\n",
      "new tweets retrieved: 233\n",
      "new tweets retrieved: 234\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "235it [00:06, 42.44it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 235\n",
      "new tweets retrieved: 236\n",
      "new tweets retrieved: 237\n",
      "new tweets retrieved: 238\n",
      "new tweets retrieved: 239\n",
      "new tweets retrieved: 240\n",
      "new tweets retrieved: 241\n",
      "new tweets retrieved: 242\n",
      "new tweets retrieved: 243\n",
      "new tweets retrieved: 244\n",
      "new tweets retrieved: 245\n",
      "new tweets retrieved: 246\n",
      "new tweets retrieved: 247\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "248it [00:06, 41.80it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 248\n",
      "new tweets retrieved: 249\n",
      "new tweets retrieved: 250\n",
      "new tweets retrieved: 251\n",
      "new tweets retrieved: 252\n",
      "new tweets retrieved: 253\n",
      "new tweets retrieved: 254\n",
      "new tweets retrieved: 255\n",
      "new tweets retrieved: 256\n",
      "new tweets retrieved: 257\n",
      "new tweets retrieved: 258\n",
      "new tweets retrieved: 259\n",
      "new tweets retrieved: 260\n",
      "new tweets retrieved: 261\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "262it [00:07, 39.93it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 262\n",
      "new tweets retrieved: 263\n",
      "new tweets retrieved: 264\n",
      "new tweets retrieved: 265\n",
      "new tweets retrieved: 266\n",
      "new tweets retrieved: 267\n",
      "new tweets retrieved: 268\n",
      "new tweets retrieved: 269\n",
      "new tweets retrieved: 270\n",
      "new tweets retrieved: 271\n",
      "new tweets retrieved: 272\n",
      "new tweets retrieved: 273\n",
      "new tweets retrieved: 274\n",
      "new tweets retrieved: 275\n",
      "new tweets retrieved: 276\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "277it [00:07, 41.58it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 277\n",
      "new tweets retrieved: 278\n",
      "new tweets retrieved: 279\n",
      "new tweets retrieved: 280\n",
      "new tweets retrieved: 281\n",
      "new tweets retrieved: 282\n",
      "new tweets retrieved: 283\n",
      "new tweets retrieved: 284\n",
      "new tweets retrieved: 285\n",
      "new tweets retrieved: 286\n",
      "new tweets retrieved: 287\n",
      "new tweets retrieved: 288\n",
      "new tweets retrieved: 289\n",
      "new tweets retrieved: 290\n",
      "new tweets retrieved: 291\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "292it [00:07, 43.72it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 292\n",
      "new tweets retrieved: 293\n",
      "new tweets retrieved: 294\n",
      "new tweets retrieved: 295\n",
      "new tweets retrieved: 296\n",
      "new tweets retrieved: 297\n",
      "new tweets retrieved: 298\n",
      "new tweets retrieved: 299\n",
      "new tweets retrieved: 300\n",
      "new tweets retrieved: 301\n",
      "new tweets retrieved: 302\n",
      "new tweets retrieved: 303\n",
      "new tweets retrieved: 304\n",
      "new tweets retrieved: 305\n",
      "new tweets retrieved: 306\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "307it [00:08, 44.38it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 307\n",
      "new tweets retrieved: 308\n",
      "new tweets retrieved: 309\n",
      "new tweets retrieved: 310\n",
      "new tweets retrieved: 311\n",
      "new tweets retrieved: 312\n",
      "new tweets retrieved: 313\n",
      "new tweets retrieved: 314\n",
      "new tweets retrieved: 315\n",
      "new tweets retrieved: 316\n",
      "new tweets retrieved: 317\n",
      "new tweets retrieved: 318\n",
      "new tweets retrieved: 319\n",
      "new tweets retrieved: 320\n",
      "new tweets retrieved: 321\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "322it [00:08, 43.86it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 322\n",
      "new tweets retrieved: 323\n",
      "new tweets retrieved: 324\n",
      "new tweets retrieved: 325\n",
      "new tweets retrieved: 326\n",
      "new tweets retrieved: 327\n",
      "new tweets retrieved: 328\n",
      "new tweets retrieved: 329\n",
      "new tweets retrieved: 330\n",
      "new tweets retrieved: 331\n",
      "new tweets retrieved: 332\n",
      "new tweets retrieved: 333\n",
      "new tweets retrieved: 334\n",
      "new tweets retrieved: 335\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "336it [00:08, 44.26it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 336\n",
      "new tweets retrieved: 337\n",
      "new tweets retrieved: 338\n",
      "new tweets retrieved: 339\n",
      "new tweets retrieved: 340\n",
      "new tweets retrieved: 341\n",
      "new tweets retrieved: 342\n",
      "new tweets retrieved: 343\n",
      "new tweets retrieved: 344\n",
      "new tweets retrieved: 345\n",
      "new tweets retrieved: 346\n",
      "new tweets retrieved: 347\n",
      "new tweets retrieved: 348\n",
      "new tweets retrieved: 349\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "350it [00:09, 44.66it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 350\n",
      "new tweets retrieved: 351\n",
      "new tweets retrieved: 352\n",
      "new tweets retrieved: 353\n",
      "new tweets retrieved: 354\n",
      "new tweets retrieved: 355\n",
      "new tweets retrieved: 356\n",
      "new tweets retrieved: 357\n",
      "new tweets retrieved: 358\n",
      "new tweets retrieved: 359\n",
      "new tweets retrieved: 360\n",
      "new tweets retrieved: 361\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "362it [00:09, 44.76it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 362\n",
      "new tweets retrieved: 363\n",
      "new tweets retrieved: 364\n",
      "new tweets retrieved: 365\n",
      "new tweets retrieved: 366\n",
      "new tweets retrieved: 367\n",
      "new tweets retrieved: 368\n",
      "new tweets retrieved: 369\n",
      "new tweets retrieved: 370\n",
      "new tweets retrieved: 371\n",
      "new tweets retrieved: 372\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "373it [00:09, 40.98it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 373\n",
      "new tweets retrieved: 374\n",
      "new tweets retrieved: 375\n",
      "new tweets retrieved: 376\n",
      "new tweets retrieved: 377\n",
      "new tweets retrieved: 378\n",
      "new tweets retrieved: 379\n",
      "new tweets retrieved: 380\n",
      "new tweets retrieved: 381\n",
      "new tweets retrieved: 382\n",
      "new tweets retrieved: 383\n",
      "new tweets retrieved: 384\n",
      "new tweets retrieved: 385\n",
      "new tweets retrieved: 386\n",
      "new tweets retrieved: 387\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "388it [00:09, 42.73it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 388\n",
      "new tweets retrieved: 389\n",
      "new tweets retrieved: 390\n",
      "new tweets retrieved: 391\n",
      "new tweets retrieved: 392\n",
      "new tweets retrieved: 393\n",
      "new tweets retrieved: 394\n",
      "new tweets retrieved: 395\n",
      "new tweets retrieved: 396\n",
      "new tweets retrieved: 397\n",
      "new tweets retrieved: 398\n",
      "new tweets retrieved: 399\n",
      "new tweets retrieved: 400\n",
      "new tweets retrieved: 401\n",
      "new tweets retrieved: 402\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "403it [00:10, 41.58it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 403\n",
      "new tweets retrieved: 404\n",
      "new tweets retrieved: 405\n",
      "new tweets retrieved: 406\n",
      "new tweets retrieved: 407\n",
      "new tweets retrieved: 408\n",
      "new tweets retrieved: 409\n",
      "new tweets retrieved: 410\n",
      "new tweets retrieved: 411\n",
      "new tweets retrieved: 412\n",
      "new tweets retrieved: 413\n",
      "new tweets retrieved: 414\n",
      "new tweets retrieved: 415\n",
      "new tweets retrieved: 416\n",
      "new tweets retrieved: 417\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "418it [00:10, 43.23it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 418\n",
      "new tweets retrieved: 419\n",
      "new tweets retrieved: 420\n",
      "new tweets retrieved: 421\n",
      "new tweets retrieved: 422\n",
      "new tweets retrieved: 423\n",
      "new tweets retrieved: 424\n",
      "new tweets retrieved: 425\n",
      "new tweets retrieved: 426\n",
      "new tweets retrieved: 427\n",
      "new tweets retrieved: 428\n",
      "new tweets retrieved: 429\n",
      "new tweets retrieved: 430\n",
      "new tweets retrieved: 431\n",
      "new tweets retrieved: 432\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "433it [00:11, 40.38it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 433\n",
      "new tweets retrieved: 434\n",
      "new tweets retrieved: 435\n",
      "new tweets retrieved: 436\n",
      "new tweets retrieved: 437\n",
      "new tweets retrieved: 438\n",
      "new tweets retrieved: 439\n",
      "new tweets retrieved: 440\n",
      "new tweets retrieved: 441\n",
      "new tweets retrieved: 442\n",
      "new tweets retrieved: 443\n",
      "new tweets retrieved: 444\n",
      "new tweets retrieved: 445\n",
      "new tweets retrieved: 446\n",
      "new tweets retrieved: 447\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "448it [00:11, 43.17it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 448\n",
      "new tweets retrieved: 449\n",
      "new tweets retrieved: 450\n",
      "new tweets retrieved: 451\n",
      "new tweets retrieved: 452\n",
      "new tweets retrieved: 453\n",
      "new tweets retrieved: 454\n",
      "new tweets retrieved: 455\n",
      "new tweets retrieved: 456\n",
      "new tweets retrieved: 457\n",
      "new tweets retrieved: 458\n",
      "new tweets retrieved: 459\n",
      "new tweets retrieved: 460\n",
      "new tweets retrieved: 461\n",
      "new tweets retrieved: 462\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "463it [00:11, 44.96it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 463\n",
      "new tweets retrieved: 464\n",
      "new tweets retrieved: 465\n",
      "new tweets retrieved: 466\n",
      "new tweets retrieved: 467\n",
      "new tweets retrieved: 468\n",
      "new tweets retrieved: 469\n",
      "new tweets retrieved: 470\n",
      "new tweets retrieved: 471\n",
      "new tweets retrieved: 472\n",
      "new tweets retrieved: 473\n",
      "new tweets retrieved: 474\n",
      "new tweets retrieved: 475\n",
      "new tweets retrieved: 476\n",
      "new tweets retrieved: 477\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "478it [00:11, 46.37it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 478\n",
      "new tweets retrieved: 479\n",
      "new tweets retrieved: 480\n",
      "new tweets retrieved: 481\n",
      "new tweets retrieved: 482\n",
      "new tweets retrieved: 483\n",
      "new tweets retrieved: 484\n",
      "new tweets retrieved: 485\n",
      "new tweets retrieved: 486\n",
      "new tweets retrieved: 487\n",
      "new tweets retrieved: 488\n",
      "new tweets retrieved: 489\n",
      "new tweets retrieved: 490\n",
      "new tweets retrieved: 491\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "500it [00:12, 40.67it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new tweets retrieved: 492\n",
      "new tweets retrieved: 493\n",
      "new tweets retrieved: 494\n",
      "new tweets retrieved: 495\n",
      "new tweets retrieved: 496\n",
      "new tweets retrieved: 497\n",
      "new tweets retrieved: 498\n",
      "new tweets retrieved: 499\n",
      "new tweets retrieved: 500\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#creation of dataframe\r\n",
    "polarity = []\r\n",
    "subjectivity = []\r\n",
    "Sentiment = []\r\n",
    "filtered_tweet = []\r\n",
    "clean_text = []\r\n",
    "country = []\r\n",
    "fltr_1 = []\r\n",
    "index = []\r\n",
    "Sentiment_rating = []\r\n",
    "\r\n",
    "tweets_df = pd.DataFrame()\r\n",
    "for tweet in tqdm(tweets_copy):\r\n",
    "    clean_text = p.clean(tweet.text)\r\n",
    "    filtered_tweet = clean_tweets(clean_text)\r\n",
    "    blob = TextBlob(filtered_tweet)\r\n",
    "    Sentiment_rating = blob.sentiment\r\n",
    "    polarity = Sentiment_rating.polarity\r\n",
    "    #splitting 1st word\r\n",
    "    #fltr_1 = filtered_tweet.split()[0]\r\n",
    "    #sentiment in text output\r\n",
    "    subjectivity = Sentiment_rating.subjectivity\r\n",
    "    if polarity >= 0.1 and polarity <= 0.3:\r\n",
    "        Sentiment = \" slightly positive\"\r\n",
    "    elif polarity < -0.1 and polarity > -0.3:\r\n",
    "        Sentiment = \"slightly negative\"\r\n",
    "    elif polarity >= 0.3:\r\n",
    "        Sentiment = \"positive\"\r\n",
    "    elif polarity < -0.3:\r\n",
    "        Sentiment = \"negative\"\r\n",
    "    else:\r\n",
    "        Sentiment = \"neutral\"\r\n",
    "        #country name (where applicable)\r\n",
    "    for country in pycountry.countries:\r\n",
    "        if country.name in tweet.user.location:\r\n",
    "            country = country.name\r\n",
    "        else:\r\n",
    "            country = \"Not Available\"\r\n",
    "            #hashtags (where applicalble)\r\n",
    "    hashtags = []\r\n",
    "    try:\r\n",
    "        for hashtag in tweet.entities[\"hashtags\"]:\r\n",
    "            hashtags.append(hashtag[\"text\"])\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    tweets_df = tweets_df.append(pd.DataFrame({'tweet_id': tweet.id,\r\n",
    "                                           'tweet_location': country,\r\n",
    "                                           'polarity': polarity,\r\n",
    "                                           'subjectivity': subjectivity,\r\n",
    "                                           'Sentiment': Sentiment,\r\n",
    "                                           'date': tweet.created_at,\r\n",
    "                                           'text': tweet.text,\r\n",
    "                                           #'filtered_tweet': fltr_1,\r\n",
    "                                            'filtered_tweet': filtered_tweet,\r\n",
    "                                           'hashtags': [hashtags if hashtags else None],\r\n",
    "                                           'source': tweet.source,\r\n",
    "                                           'likes': tweet.favorite_count,\r\n",
    "                                           'is_retweet': tweet.retweeted}, index=[0]))\r\n",
    "    tweets_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23492/2082761439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                            \u001b[1;34m'source'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                                            \u001b[1;34m'likes'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfavorite_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                                            'is_retweet': tweet.retweeted}, index=[0]))\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    391\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# from BlockManager perspective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    275\u001b[0m                 \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             val = sanitize_array(val, index, dtype=dtype, copy=False,\n\u001b[1;32m--> 277\u001b[1;33m                                  raise_cast_failure=False)\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[0mhomogenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             subarr = construct_1d_arraylike_from_scalar(\n\u001b[1;32m--> 642\u001b[1;33m                 value, len(index), dtype)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[1;34m(value, length, dtype)\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#EXPORTING DATAFRAME ONTO GOOGLESHEETS\r\n",
    "client= pygsheets.authorize(service_file='filepath')\r\n",
    "lbl.configure(text=\"Authorized\")\r\n",
    "\r\n",
    "sheet = client.open(\"fyp_tw\")\r\n",
    "prgbar.configure(text=\"Current status: Sheet opened\")\r\n",
    "\r\n",
    "wks = sheet[0]\r\n",
    "prgbar.configure(text=\"Current status: First sheet Accessed\")\r\n",
    "\r\n",
    "wks.set_dataframe(tweets_df,(1,1))\r\n",
    "prgbar.configure(text=\"Current status: Data Updated\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'filepath'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23492/2071510687.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#EXPORTING DATAFRAME ONTO GOOGLESHEETS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpygsheets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'filepath'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlbl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Authorized\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msheet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fyp_tw\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\pygsheets\\authorization.py\u001b[0m in \u001b[0;36mauthorize\u001b[1;34m(client_secret, service_account_file, service_account_env_var, credentials_directory, scopes, custom_credentials, local, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             service_account_info, scopes=scopes)\n\u001b[0;32m    126\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mservice_account_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mcredentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_service_account_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice_account_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mcredentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_user_authentication_credentials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient_secret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\google\\oauth2\\service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \"\"\"\n\u001b[0;32m    233\u001b[0m         info, signer = _service_account_info.from_filename(\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"client_email\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"token_uri\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         )\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_signer_and_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ml2env\\lib\\site-packages\\google\\auth\\_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0msigner\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'filepath'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "webbrowser.open(\"your public tableau link\")\r\n",
    "\r\n",
    "btn = Button(tab1, text=\"Click Me to start Analysis\", command=clicked)\r\n",
    "\r\n",
    "btn.grid(column=3, row=10)\r\n",
    "tab_control.pack(expand=1, fill='both')\r\n",
    "\r\n",
    "window.mainloop()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('ml2env': conda)"
  },
  "interpreter": {
   "hash": "4bc1484d5d5144b78f47b52320ed3d66b9b7a0f64abfbdce1ade28afcf51bab3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}